{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-608ed929f2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mWe\u001b[0m \u001b[0malso\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muse\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named utils"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text.  Instead of T(s, a, s') being  probability number for each\n",
    "    state/action/state triplet, we instead have T(s, a) return a list of (p, s')\n",
    "    pairs.  We also keep track of the possible states, terminal states, and\n",
    "    actions for each state. [page 615]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
    "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
    "               gamma=gamma, states=set(), reward={})\n",
    "\n",
    "    def R(self, state):\n",
    "        \"Return a numeric reward for this state.\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(state, action):\n",
    "        \"\"\"Transition model.  From a state and an action, return a list\n",
    "        of (result-state, probability) pairs.\"\"\"\n",
    "        abstract\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state).  Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse() ## because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action == None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"Return the state that results from going in this direction.\"\n",
    "        state1 = vector_add(state, direction)\n",
    "        return if_(state1 in self.states, state1, state)\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x,y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0):'>', (0, 1):'^', (-1, 0):'<', (0, -1):'v', None: '.'}\n",
    "        return self.to_grid(dict([(s, chars[a]) for (s, a) in policy.items()]))\n",
    "\n",
    "\n",
    "Fig[17,1] = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                     [-0.04, None,  -0.04, -1],\n",
    "                     [-0.04, -0.04, -0.04, -0.04]],\n",
    "                    terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"Solving an MDP by value iteration. [Fig. 17.4]\"\n",
    "    U1 = dict([(s, 0) for s in mdp.states])\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "             return U\n",
    "\n",
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = argmax(mdp.actions(s), lambda a:expected_utility(a, s, U, mdp))\n",
    "    return pi\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    \"The expected utility of doing a in state s, according to the MDP and U.\"\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    \"Solve an MDP by policy iteration [Fig. 17.7]\"\n",
    "    U = dict([(s, 0) for s in mdp.states])\n",
    "    pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = argmax(mdp.actions(s), lambda a: expected_utility(a,s,U,mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi\n",
    "\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s] for (p, s1) in T(s, pi[s])])\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
